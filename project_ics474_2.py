# -*- coding: utf-8 -*-
"""project_ics474-2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/gist/Meshal-alfaifi/2e71b2276cd005b49569a1d6ffafbda8/project_ics474-2.ipynb
"""

# @title 1. FIXED Install & Setup Infrastructure
import os
import subprocess
import time

print("‚è≥ Starting Robust Installation... (This takes ~2 mins)")

# 1. Update Apt Repo & Install Java (Fixed for Colab 2025)
!apt-get update -qq > /dev/null
!apt-get install default-jdk-headless -qq > /dev/null
# Set Java Home to the default location found in Colab
os.environ["JAVA_HOME"] = "/usr/lib/jvm/default-java"
print("‚úÖ Java installed.")

# 2. Install Python Libraries
!pip install pyspark==3.5.0 kafka-python pandas scikit-learn fastapi uvicorn streamlit pydeck pyarrow psycopg2-binary sqlalchemy pyngrok -q
print("‚úÖ Python libraries installed.")

# 3. Install & Start PostgreSQL
!sudo apt-get -y -q install postgresql > /dev/null
!service postgresql start
# Setup User and Database
!sudo -u postgres psql -c "CREATE USER root WITH SUPERUSER PASSWORD 'password';"
!sudo -u postgres createdb traffic_db
print("‚úÖ PostgreSQL started & Database 'traffic_db' created.")

# 4. Download & Start Kafka (Using Archive to prevent 404s)
kafka_version = "3.7.0"
kafka_file = f"kafka_2.12-{kafka_version}.tgz"
kafka_url = f"https://archive.apache.org/dist/kafka/{kafka_version}/{kafka_file}"

if not os.path.exists(kafka_file):
    print(f"‚¨áÔ∏è Downloading Kafka {kafka_version}...")
    !wget -q {kafka_url}

if os.path.exists(kafka_file):
    !tar -xzf {kafka_file}
    kafka_home = f"kafka_2.12-{kafka_version}"
    print(f"‚úÖ Kafka downloaded and extracted to {kafka_home}")

    # Start Zookeeper (Background)
    print("‚è≥ Starting Zookeeper...")
    subprocess.Popen([f"{kafka_home}/bin/zookeeper-server-start.sh", f"{kafka_home}/config/zookeeper.properties"], stdout=subprocess.DEVNULL)
    time.sleep(5)

    # Start Kafka (Background)
    print("‚è≥ Starting Kafka Broker...")
    subprocess.Popen([f"{kafka_home}/bin/kafka-server-start.sh", f"{kafka_home}/config/server.properties"], stdout=subprocess.DEVNULL)
    time.sleep(10)

    # Create the 'traffic' topic
    print("‚è≥ Creating 'traffic' Topic...")
    !{kafka_home}/bin/kafka-topics.sh --create --topic traffic --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 2>/dev/null
    print("‚úÖ Infrastructure Setup Complete!")
else:
    print("‚ùå Error: Kafka failed to download. Please check the URL.")

# @title 2. Run Data Pipeline (Ingest -> Kafka -> DB/Parquet)
import json
import random
import time
import pandas as pd
from kafka import KafkaProducer
from sqlalchemy import create_engine
import os

# 1. Setup Connections
producer = KafkaProducer(bootstrap_servers=['localhost:9092'], value_serializer=lambda x: json.dumps(x).encode('utf-8'))
db_engine = create_engine('postgresql+psycopg2://root:password@localhost:5432/traffic_db')

print("üöÄ Pipeline Active: Generating traffic data for 60 seconds...")
print("üìä Storage Targets: [1] PostgreSQL Table  [2] Parquet Data Lake")

start_time = time.time()
records_buffer = []

# Loop for 60 seconds
while time.time() - start_time < 60:
    # A. Simulate IoT Data (The "Sensor")
    data = {
        "sensor_id": random.choice(["Main_St_North", "Main_St_South", "Highway_40_East"]),
        "timestamp": int(time.time()),
        "speed": random.randint(20, 120),  # Random speed
        "vehicle_count": random.randint(1, 50), # Traffic density
        "temperature": random.randint(25, 45) # Environmental data
    }

    # B. Send to Kafka (The "Broker")
    producer.send('traffic', value=data)

    # C. Consume & Store (The "Processing Layer")
    # In a real app, this part would be in a separate script/cell
    records_buffer.append(data)

    # Batch save every 10 records
    if len(records_buffer) >= 10:
        df = pd.DataFrame(records_buffer)

        # 1. Save to PostgreSQL (Requirement)
        df.to_sql('traffic_data', db_engine, if_exists='append', index=False)

        # 2. Save to Parquet (Bonus: Data Lake)
        os.makedirs('data/bronze', exist_ok=True)
        df.to_parquet(f'data/bronze/traffic_{int(time.time())}.parquet')

        print(f"üì¶ Batch Processed: {len(df)} records saved to DB & Lake.")
        records_buffer = [] # Clear buffer

    time.sleep(0.5) # Simulate slight delay between sensor readings

print("‚úÖ Data Generation Complete. Pipeline stopped.")

# @title 2B. Processing Layer (PySpark) ‚Äî Bronze Parquet -> Silver Aggregates
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, from_unixtime, to_timestamp, window, avg, count

spark = SparkSession.builder.appName("FlowSight-Processing-Layer").getOrCreate()

bronze_path = "data/bronze"
df_bronze = spark.read.parquet(bronze_path)

# Convert epoch seconds -> timestamp
df_bronze = df_bronze.withColumn("event_time", to_timestamp(from_unixtime(col("timestamp"))))

# 1-minute per-sensor aggregation (use sensor_id, not sensor)
df_silver = (
    df_bronze
    .groupBy(window(col("event_time"), "1 minute").alias("w"), col("sensor_id"))
    .agg(
        avg(col("speed")).alias("avg_speed_1m"),
        count("*").alias("events_1m")
    )
    .select(
        col("sensor_id"),
        col("w.start").alias("window_start"),
        col("w.end").alias("window_end"),
        col("avg_speed_1m"),
        col("events_1m")
    )
)

print("‚úÖ Spark processing complete (sample):")
df_silver.show(10, truncate=False)

silver_path = "data/silver"
df_silver.write.mode("overwrite").parquet(silver_path)
print(f"‚úÖ Wrote SILVER Parquet to: {silver_path}")

# @title 3. Train AI Model (Random Forest)
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error
import joblib

print("üß† fetching training data from PostgreSQL...")

# 1. Read from PostgreSQL
db_engine = create_engine('postgresql+psycopg2://root:password@localhost:5432/traffic_db')
df = pd.read_sql("SELECT * FROM traffic_data", db_engine)

if len(df) == 0:
    print("‚ùå No data found! Did you run Step 2?")
else:
    # 2. Prepare Features (X) and Target (y)
    # We try to predict 'speed' based on 'vehicle_count' and 'temperature'
    X = df[['vehicle_count', 'temperature']]
    y = df['speed']

    # 3. Train Model
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    model = RandomForestRegressor(n_estimators=100)
    model.fit(X_train, y_train)

    # 4. Evaluate
    predictions = model.predict(X_test)
    mae = mean_absolute_error(y_test, predictions)

    # 5. Save Model
    joblib.dump(model, 'traffic_model.pkl')

    print(f"‚úÖ Model Trained Successfully on {len(df)} records.")
    print(f"üìä Model Performance (MAE): {mae:.2f}")
    print("üíæ Model saved as 'traffic_model.pkl'")

# # @title 4. Launch Streamlit Dashboard
# from pyngrok import ngrok

# # 1. Create the Dashboard Application File
# %%writefile app.py
# import streamlit as st
# import pandas as pd
# from sqlalchemy import create_engine
# import joblib
# import pydeck as pdk
# import time

# # Page Config
# st.set_page_config(page_title="FlowSight Traffic AI", layout="wide")
# st.title("üö¶ FlowSight: Real-Time Traffic AI")

# # Sidebar
# st.sidebar.header("Control Panel")
# refresh_rate = st.sidebar.slider("Refresh Rate (seconds)", 1, 60, 5)

# # Connect to DB
# engine = create_engine('postgresql+psycopg2://root:password@localhost:5432/traffic_db')

# # Top Metrics Row
# col1, col2, col3 = st.columns(3)

# # Load Data
# try:
#     df = pd.read_sql("SELECT * FROM traffic_data ORDER BY timestamp DESC LIMIT 100", engine)

#     # Display Metrics
#     latest = df.iloc[0]
#     col1.metric("Latest Sensor Speed", f"{latest['speed']} km/h")
#     col2.metric("Vehicle Count", f"{latest['vehicle_count']}")
#     col3.metric("System Status", "Online üü¢")

#     # Map Visualization (Analytical Depth Bonus)
#     st.subheader("üó∫Ô∏è Live Sensor Network (Dammam Simulation)")

#     # Simulating coordinates for the demo (Dammam area)
#     # In a real app, these would come from the 'sensor_id' metadata
#     map_data = df.head(10).copy()
#     map_data['lat'] = [26.4207 + (i*0.002) for i in range(len(map_data))]
#     map_data['lon'] = [50.0888 + (i*0.002) for i in range(len(map_data))]

#     st.pydeck_chart(pdk.Deck(
#         map_style='mapbox://styles/mapbox/dark-v10',
#         initial_view_state=pdk.ViewState(latitude=26.43, longitude=50.10, zoom=12, pitch=50),
#         layers=[
#             pdk.Layer(
#                 'ColumnLayer',
#                 data=map_data,
#                 get_position='[lon, lat]',
#                 get_elevation='vehicle_count',
#                 elevation_scale=50,
#                 radius=100,
#                 get_fill_color='[255, 165, 0, 140]',
#                 pickable=True,
#                 auto_highlight=True,
#             ),
#         ],
#     ))

#     # AI Prediction Section
#     st.divider()
#     st.subheader("ü§ñ AI Congestion Predictor")

#     c1, c2, c3 = st.columns([1,1,2])
#     with c1:
#         input_count = st.number_input("Input Vehicle Count", 0, 100, 30)
#     with c2:
#         input_temp = st.number_input("Input Temperature (¬∞C)", 0, 50, 35)

#     if st.button("Predict Speed"):
#         model = joblib.load('traffic_model.pkl')
#         pred = model.predict([[input_count, input_temp]])
#         st.success(f"Predicted Traffic Speed: {pred[0]:.2f} km/h")

# except Exception as e:
#     st.error("Waiting for data pipeline... Run Step 2!")

# # Auto-refresh logic (Experimental)
# time.sleep(refresh_rate)
# st.rerun()

# # --- End of app.py ---

# Commented out IPython magic to ensure Python compatibility.
# # @title 4. Launch Streamlit Dashboard
# %%writefile app.py
# import streamlit as st
# import pandas as pd
# from sqlalchemy import create_engine
# import joblib
# import pydeck as pdk
# import time
# 
# # Page Config
# st.set_page_config(page_title="FlowSight Traffic AI", layout="wide")
# st.title("üö¶ FlowSight: Real-Time Traffic AI")
# 
# # Sidebar
# st.sidebar.header("Control Panel")
# refresh_rate = st.sidebar.slider("Refresh Rate (seconds)", 1, 60, 5)
# 
# # Connect to DB
# # Note: In Colab, localhost works because Streamlit runs on the same VM
# engine = create_engine('postgresql+psycopg2://root:password@localhost:5432/traffic_db')
# 
# try:
#     # Top Metrics Row
#     col1, col2, col3 = st.columns(3)
# 
#     # Load Data
#     df = pd.read_sql("SELECT * FROM traffic_data ORDER BY timestamp DESC LIMIT 100", engine)
# 
#     if not df.empty:
#         # Display Metrics
#         latest = df.iloc[0]
#         col1.metric("Latest Sensor Speed", f"{latest['speed']} km/h")
#         col2.metric("Vehicle Count", f"{latest['vehicle_count']}")
#         col3.metric("System Status", "Online üü¢")
# 
#         # Map Visualization (Analytical Depth Bonus)
#         st.subheader("üó∫Ô∏è Live Sensor Network (Dammam Simulation)")
# 
#         # Simulating coordinates for the demo (Dammam area)
#         map_data = df.head(50).copy()
#         # Add slight spread to coords so they don't stack perfectly
#         map_data['lat'] = [26.4207 + (i*0.0005) for i in range(len(map_data))]
#         map_data['lon'] = [50.0888 + (i*0.0005) for i in range(len(map_data))]
# 
#         st.pydeck_chart(pdk.Deck(
#             map_style='mapbox://styles/mapbox/dark-v10',
#             initial_view_state=pdk.ViewState(latitude=26.43, longitude=50.10, zoom=13, pitch=50),
#             layers=[
#                 pdk.Layer(
#                     'ColumnLayer',
#                     data=map_data,
#                     get_position='[lon, lat]',
#                     get_elevation='vehicle_count',
#                     elevation_scale=10,
#                     radius=50,
#                     get_fill_color='[255, 165, 0, 140]',
#                     pickable=True,
#                     auto_highlight=True,
#                 ),
#             ],
#         ))
# 
#         # AI Prediction Section
#         st.divider()
#         st.subheader("ü§ñ AI Congestion Predictor")
# 
#         c1, c2, c3 = st.columns([1,1,2])
#         with c1:
#             input_count = st.number_input("Input Vehicle Count", 0, 100, 30)
#         with c2:
#             input_temp = st.number_input("Input Temperature (¬∞C)", 0, 50, 35)
# 
#         if st.button("Predict Speed"):
#             try:
#                 model = joblib.load('traffic_model.pkl')
#                 pred = model.predict([[input_count, input_temp]])
#                 st.success(f"Predicted Traffic Speed: {pred[0]:.2f} km/h")
#             except:
#                 st.warning("Model not found yet. Run Step 3 first!")
#     else:
#         st.warning("Waiting for data... Ensure Step 2 is running!")
# 
# except Exception as e:
#     st.error(f"Connection Error: {e}")
# 
# # Auto-refresh logic
# time.sleep(refresh_rate)
# st.rerun()

from pyngrok import ngrok

# 1. Terminate any stuck tunnels
ngrok.kill()

# 2. Set your token (Get one from https://dashboard.ngrok.com/get-started/your-authtoken)
NGROK_AUTH_TOKEN = "35yzMVUgAAGxCthVMYtdW9Pd7MU_4x1e4foUHoETSNKT8ZSC5"  # <---- PASTE TOKEN HERE
ngrok.set_auth_token(NGROK_AUTH_TOKEN)

# 3. Open the Tunnel to port 8501 (Streamlit's default port)
try:
    public_url = ngrok.connect(8501).public_url
    print(f"üöÄ DASHBOARD IS LIVE! Click here: {public_url}")
except Exception as e:
    print(f"Ngrok Error: {e}")

# 4. Run Streamlit in the background
!streamlit run app.py &>/dev/null&

# @title 5. Inspect PostgreSQL Data (Evidence)
import pandas as pd
from sqlalchemy import create_engine

# Connect to the local Colab Database
engine = create_engine('postgresql+psycopg2://root:password@localhost:5432/traffic_db')

# Run a standard SQL Query
print("üîç Checking 'traffic_data' table in PostgreSQL...")
try:
    df = pd.read_sql("SELECT * FROM traffic_data ORDER BY timestamp DESC LIMIT 10", engine)
    display(df) # This prints a nice table
    print(f"\n‚úÖ SUCCESS: Found {len(df)} records in the Database.")
except Exception as e:
    print(f"‚ùå Error: {e}")